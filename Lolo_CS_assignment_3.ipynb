{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>album</th>\n",
       "      <th>artist</th>\n",
       "      <th>audio_features</th>\n",
       "      <th>context</th>\n",
       "      <th>decades</th>\n",
       "      <th>genres</th>\n",
       "      <th>lyrics_features</th>\n",
       "      <th>moods</th>\n",
       "      <th>name</th>\n",
       "      <th>new_context</th>\n",
       "      <th>picture</th>\n",
       "      <th>recording_id</th>\n",
       "      <th>sub_context</th>\n",
       "      <th>yt_id</th>\n",
       "      <th>yt_views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'$oid': '52fdfb440b9398049f3d7a8c'}</td>\n",
       "      <td>Gangnam Style (강남스타일)</td>\n",
       "      <td>PSY</td>\n",
       "      <td>[11, 0.912744, 0.083704, 132.069, 0.293137, 0....</td>\n",
       "      <td>[work out]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[pop]</td>\n",
       "      <td>[oppa, gangnam, style, gangnam, style, najeneu...</td>\n",
       "      <td>[energetic, motivational]</td>\n",
       "      <td>Gangnam Style (강남스타일)</td>\n",
       "      <td>work out</td>\n",
       "      <td>http://images.musicnet.com/albums/073/463/405/...</td>\n",
       "      <td>50232.0</td>\n",
       "      <td>[working out: cardio]</td>\n",
       "      <td>9bZkp7q19f0</td>\n",
       "      <td>2450112089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'$oid': '52fdfb3d0b9398049f3cbc8e'}</td>\n",
       "      <td>Native</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>[6, 0.7457039999999999, 0.11995499999999999, 1...</td>\n",
       "      <td>[energetic]</td>\n",
       "      <td>[2012]</td>\n",
       "      <td>[pop]</td>\n",
       "      <td>[lately, i, ve, been, i, ve, been, losing, sle...</td>\n",
       "      <td>[happy]</td>\n",
       "      <td>Counting Stars</td>\n",
       "      <td>energetic</td>\n",
       "      <td>http://images.musicnet.com/albums/081/851/887/...</td>\n",
       "      <td>5839.0</td>\n",
       "      <td>[energy boost]</td>\n",
       "      <td>hT_nvWreIhg</td>\n",
       "      <td>1020297206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'$oid': '52fdfb420b9398049f3d3ea5'}</td>\n",
       "      <td>Party Rock Anthem</td>\n",
       "      <td>LMFAO</td>\n",
       "      <td>[5, 0.709932, 0.231455, 130.03, 0.121740999999...</td>\n",
       "      <td>[energetic, energetic, energetic, energetic]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[party, rock, yeah, woo, let, s, go, party, ro...</td>\n",
       "      <td>[happy, celebratory, rowdy]</td>\n",
       "      <td>Party Rock Anthem</td>\n",
       "      <td>housework</td>\n",
       "      <td>http://images.musicnet.com/albums/049/414/127/...</td>\n",
       "      <td>52379.0</td>\n",
       "      <td>[energy boost, pleasing a crowd, housework, dr...</td>\n",
       "      <td>KQ6zr6kCPj8</td>\n",
       "      <td>971128436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    _id                  album       artist  \\\n",
       "0  {'$oid': '52fdfb440b9398049f3d7a8c'}  Gangnam Style (강남스타일)          PSY   \n",
       "1  {'$oid': '52fdfb3d0b9398049f3cbc8e'}                 Native  OneRepublic   \n",
       "2  {'$oid': '52fdfb420b9398049f3d3ea5'}      Party Rock Anthem        LMFAO   \n",
       "\n",
       "                                      audio_features  \\\n",
       "0  [11, 0.912744, 0.083704, 132.069, 0.293137, 0....   \n",
       "1  [6, 0.7457039999999999, 0.11995499999999999, 1...   \n",
       "2  [5, 0.709932, 0.231455, 130.03, 0.121740999999...   \n",
       "\n",
       "                                        context decades genres  \\\n",
       "0                                    [work out]      []  [pop]   \n",
       "1                                   [energetic]  [2012]  [pop]   \n",
       "2  [energetic, energetic, energetic, energetic]      []     []   \n",
       "\n",
       "                                     lyrics_features  \\\n",
       "0  [oppa, gangnam, style, gangnam, style, najeneu...   \n",
       "1  [lately, i, ve, been, i, ve, been, losing, sle...   \n",
       "2  [party, rock, yeah, woo, let, s, go, party, ro...   \n",
       "\n",
       "                         moods                   name new_context  \\\n",
       "0    [energetic, motivational]  Gangnam Style (강남스타일)    work out   \n",
       "1                      [happy]         Counting Stars   energetic   \n",
       "2  [happy, celebratory, rowdy]      Party Rock Anthem   housework   \n",
       "\n",
       "                                             picture  recording_id  \\\n",
       "0  http://images.musicnet.com/albums/073/463/405/...       50232.0   \n",
       "1  http://images.musicnet.com/albums/081/851/887/...        5839.0   \n",
       "2  http://images.musicnet.com/albums/049/414/127/...       52379.0   \n",
       "\n",
       "                                         sub_context        yt_id    yt_views  \n",
       "0                              [working out: cardio]  9bZkp7q19f0  2450112089  \n",
       "1                                     [energy boost]  hT_nvWreIhg  1020297206  \n",
       "2  [energy boost, pleasing a crowd, housework, dr...  KQ6zr6kCPj8   971128436  "
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_df = pd.read_json('MasterSongList.json')\n",
    "songs_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove empty lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only keep lyrics and moods in our new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics_features</th>\n",
       "      <th>moods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[oppa, gangnam, style, gangnam, style, najeneu...</td>\n",
       "      <td>[energetic, motivational]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[lately, i, ve, been, i, ve, been, losing, sle...</td>\n",
       "      <td>[happy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[party, rock, yeah, woo, let, s, go, party, ro...</td>\n",
       "      <td>[happy, celebratory, rowdy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[alagamun, lan, weh, wakun, heya, hanun, gon, ...</td>\n",
       "      <td>[happy, energetic, celebratory]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[j, lo, the, other, side, out, my, mine, it, s...</td>\n",
       "      <td>[energetic]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     lyrics_features  \\\n",
       "0  [oppa, gangnam, style, gangnam, style, najeneu...   \n",
       "1  [lately, i, ve, been, i, ve, been, losing, sle...   \n",
       "2  [party, rock, yeah, woo, let, s, go, party, ro...   \n",
       "3  [alagamun, lan, weh, wakun, heya, hanun, gon, ...   \n",
       "4  [j, lo, the, other, side, out, my, mine, it, s...   \n",
       "\n",
       "                             moods  \n",
       "0        [energetic, motivational]  \n",
       "1                          [happy]  \n",
       "2      [happy, celebratory, rowdy]  \n",
       "3  [happy, energetic, celebratory]  \n",
       "4                      [energetic]  "
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['lyrics_features', 'moods']\n",
    "lyrics_df = songs_df.copy()\n",
    "lyrics_df = lyrics_df[cols]\n",
    "lyrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the list format for both columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics_features</th>\n",
       "      <th>moods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oppa gangnam style gangnam style najeneun ttas...</td>\n",
       "      <td>energetic, motivational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lately i ve been i ve been losing sleep dreami...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>party rock yeah woo let s go party rock is in ...</td>\n",
       "      <td>happy, celebratory, rowdy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alagamun lan weh wakun heya hanun gon alagamun...</td>\n",
       "      <td>happy, energetic, celebratory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j lo the other side out my mine it s a new gen...</td>\n",
       "      <td>energetic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     lyrics_features  \\\n",
       "0  oppa gangnam style gangnam style najeneun ttas...   \n",
       "1  lately i ve been i ve been losing sleep dreami...   \n",
       "2  party rock yeah woo let s go party rock is in ...   \n",
       "3  alagamun lan weh wakun heya hanun gon alagamun...   \n",
       "4  j lo the other side out my mine it s a new gen...   \n",
       "\n",
       "                           moods  \n",
       "0        energetic, motivational  \n",
       "1                          happy  \n",
       "2      happy, celebratory, rowdy  \n",
       "3  happy, energetic, celebratory  \n",
       "4                      energetic  "
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_df['lyrics_features'] = lyrics_df['lyrics_features'].apply(' '.join)\n",
    "lyrics_df['moods'] = lyrics_df['moods'].apply(', '.join)\n",
    "lyrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now replace the empty lyrics songs with NaN and drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36733, 2)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_df['lyrics_features'].replace('', np.nan, inplace=True)\n",
    "lyrics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20931, 2)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_df.dropna(subset=['lyrics_features'], inplace=True)\n",
    "lyrics_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step is to re-index the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before choosing any moods, let's clean up the lyrics (as we will need to use all of them for the Doc2Vec method later on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convert to lower case\n",
    "2. Remove punctuation\n",
    "3. Remove common words\n",
    "4. Stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw_text):\n",
    "    # Create empty list to receive result\n",
    "    clean_words = []\n",
    "    \n",
    "    # 1. Convert to lower case\n",
    "    raw_text = raw_text.lower()\n",
    "    \n",
    "    # 2. Remove punctuation\n",
    "    translator = str.maketrans('', '', punctuation)\n",
    "    raw_text = raw_text.translate(translator)\n",
    "    split_words = raw_text.split()\n",
    "    \n",
    "    # 3 & 4. Remove common words and stem words\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    for word in split_words:\n",
    "        if word not in ENGLISH_STOP_WORDS:\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            clean_words.append(stemmed_word)\n",
    "            \n",
    "    return ' '.join(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this function to all our lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df['clean_lyrics'] = lyrics_df['lyrics_features'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lately i ve been i ve been losing sleep dreaming about the things that we could be but baby i ve been i ve been praying hard said no more counting dollars we ll be counting stars yeah we ll be counting stars i see this life like a swinging vine swing my heart across the line and my face is flashing signs seek it out and you shall find old but i m not that old young but i m not that bold i don t think the world is sold i m just doing what we re told i feel something so right doing the wrong thing\n"
     ]
    }
   ],
   "source": [
    "print(lyrics_df['lyrics_features'][1][0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "late ve ve lose sleep dream thing babi ve ve pray hard said count dollar ll count star yeah ll count star life like swing vine swing heart line face flash sign seek shall old m old young m bold don t think world sold m just do told feel right do wrong thing feel wrong do right thing couldn t lie couldn t lie couldn t lie kill make feel aliv late ve ve lose sleep dream thing babi ve ve pray hard said count dollar ll count star late ve ve lose sleep dream thing babi ve ve pray hard said count doll\n"
     ]
    }
   ],
   "source": [
    "print(lyrics_df['clean_lyrics'][1][0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing moods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at all the moods available: we use the initial dataframe as the moods are still in a list. A set will allow to show all unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aggressive',\n",
       " 'angsty',\n",
       " 'atmospheric',\n",
       " 'campy',\n",
       " 'celebratory',\n",
       " 'classy',\n",
       " 'cocky',\n",
       " 'cold',\n",
       " 'earthy',\n",
       " 'energetic',\n",
       " 'funky',\n",
       " 'gloomy',\n",
       " 'happy',\n",
       " 'hypnotic',\n",
       " 'introspective',\n",
       " 'lush',\n",
       " 'mellow',\n",
       " 'motivational',\n",
       " 'nocturnal',\n",
       " 'raw',\n",
       " 'rowdy',\n",
       " 'sad',\n",
       " 'seductive',\n",
       " 'sexual',\n",
       " 'soothing',\n",
       " 'spacey',\n",
       " 'sprightly',\n",
       " 'sweet',\n",
       " 'trashy',\n",
       " 'trippy',\n",
       " 'visceral',\n",
       " 'warm'}"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moods = songs_df['moods'].tolist()\n",
    "moods_set = set(x for i in moods for x in i)\n",
    "moods_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before choosing which mood to use, let's have a look at their distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "funky :  2072\n",
      "hypnotic :  286\n",
      "rowdy :  1721\n",
      "happy :  1757\n",
      "atmospheric :  1155\n",
      "motivational :  925\n",
      "campy :  636\n",
      "cold :  830\n",
      "introspective :  1417\n",
      "sweet :  814\n",
      "sprightly :  1733\n",
      "lush :  1326\n",
      "soothing :  1374\n",
      "mellow :  2856\n",
      "seductive :  1419\n",
      "trippy :  780\n",
      "trashy :  477\n",
      "energetic :  2305\n",
      "sad :  1249\n",
      "earthy :  873\n",
      "nocturnal :  1334\n",
      "angsty :  1205\n",
      "cocky :  1438\n",
      "visceral :  1112\n",
      "spacey :  514\n",
      "raw :  1301\n",
      "aggressive :  1683\n",
      "sexual :  505\n",
      "warm :  1495\n",
      "classy :  492\n",
      "gloomy :  750\n",
      "celebratory :  1479\n"
     ]
    }
   ],
   "source": [
    "def number_moods(mood):\n",
    "    count = len(lyrics_df[lyrics_df['moods'].str.contains(mood)])\n",
    "    return count\n",
    "\n",
    "for i in moods_set:\n",
    "    mood_count = number_moods(i)\n",
    "    print(i, \": \", mood_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we decide to choose 'aggressive' and 'mellow', are there songs that fit both moods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [lyrics_features, moods, clean_lyrics]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(lyrics_df[lyrics_df['moods'].str.contains('aggressive') & lyrics_df['moods'].str.contains('mellow')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, there is no overlapping data for those 2 moods. We will use those 2 moods for Bag of Words and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "mellow_aggressive_df = lyrics_df.copy()\n",
    "\n",
    "aggressive_condition = mellow_aggressive_df['moods'].str.contains('aggressive')\n",
    "mellow_condition = mellow_aggressive_df['moods'].str.contains('mellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "mellow_aggressive_df = mellow_aggressive_df[aggressive_condition | mellow_condition]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier, let's make sure the mood column contains either mellow or aggressive, but no other mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "mellow_aggressive_df.loc[aggressive_condition, 'moods'] = 'aggressive'\n",
    "mellow_aggressive_df.loc[mellow_condition, 'moods'] = 'mellow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics_features</th>\n",
       "      <th>moods</th>\n",
       "      <th>clean_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1 and if you ll see andmoreagain then you will...</td>\n",
       "      <td>mellow</td>\n",
       "      <td>1 ll andmoreagain know andmoreagain eye feel h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>wake up wake up grab a brush and put a little ...</td>\n",
       "      <td>aggressive</td>\n",
       "      <td>wake wake grab brush littl makeup hide scar fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>kiss me hard before you go summertime sadness ...</td>\n",
       "      <td>mellow</td>\n",
       "      <td>kiss hard summertim sad just want know babi be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>well you done done me in you bet i felt it i t...</td>\n",
       "      <td>mellow</td>\n",
       "      <td>bet felt tri chill hot melt fell right crack m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>now greetings to the world vice ala one big go...</td>\n",
       "      <td>aggressive</td>\n",
       "      <td>greet world vice ala big gong zilla longsid sk...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       lyrics_features       moods  \\\n",
       "37   1 and if you ll see andmoreagain then you will...      mellow   \n",
       "46   wake up wake up grab a brush and put a little ...  aggressive   \n",
       "74   kiss me hard before you go summertime sadness ...      mellow   \n",
       "76   well you done done me in you bet i felt it i t...      mellow   \n",
       "121  now greetings to the world vice ala one big go...  aggressive   \n",
       "\n",
       "                                          clean_lyrics  \n",
       "37   1 ll andmoreagain know andmoreagain eye feel h...  \n",
       "46   wake wake grab brush littl makeup hide scar fa...  \n",
       "74   kiss hard summertim sad just want know babi be...  \n",
       "76   bet felt tri chill hot melt fell right crack m...  \n",
       "121  greet world vice ala big gong zilla longsid sk...  "
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mellow_aggressive_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mellow        2856\n",
       "aggressive    1683\n",
       "Name: moods, dtype: int64"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mellow_aggressive_df['moods'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we notice there are more mellow songs than aggressive ones, let's balance the dataframe to make sure this does not have an influence on our result. We will then randomly select 1683 mellow songs (to match the aggressive ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggressive_songs = mellow_aggressive_df[mellow_aggressive_df['moods'] == 'aggressive']\n",
    "mellow_songs = mellow_aggressive_df[mellow_aggressive_df['moods'] == 'mellow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1683, 3)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mellow_sample = mellow_songs.sample(n=len(aggressive_songs), random_state=101)\n",
    "mellow_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now concatenate back into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics_features</th>\n",
       "      <th>moods</th>\n",
       "      <th>clean_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1 and if you ll see andmoreagain then you will...</td>\n",
       "      <td>mellow</td>\n",
       "      <td>1 ll andmoreagain know andmoreagain eye feel h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>wake up wake up grab a brush and put a little ...</td>\n",
       "      <td>aggressive</td>\n",
       "      <td>wake wake grab brush littl makeup hide scar fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>well you done done me in you bet i felt it i t...</td>\n",
       "      <td>mellow</td>\n",
       "      <td>bet felt tri chill hot melt fell right crack m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>now greetings to the world vice ala one big go...</td>\n",
       "      <td>aggressive</td>\n",
       "      <td>greet world vice ala big gong zilla longsid sk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>the secret side of me i never let you see i ke...</td>\n",
       "      <td>aggressive</td>\n",
       "      <td>secret let cage t control stay away beast ugli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       lyrics_features       moods  \\\n",
       "37   1 and if you ll see andmoreagain then you will...      mellow   \n",
       "46   wake up wake up grab a brush and put a little ...  aggressive   \n",
       "76   well you done done me in you bet i felt it i t...      mellow   \n",
       "121  now greetings to the world vice ala one big go...  aggressive   \n",
       "175  the secret side of me i never let you see i ke...  aggressive   \n",
       "\n",
       "                                          clean_lyrics  \n",
       "37   1 ll andmoreagain know andmoreagain eye feel h...  \n",
       "46   wake wake grab brush littl makeup hide scar fa...  \n",
       "76   bet felt tri chill hot melt fell right crack m...  \n",
       "121  greet world vice ala big gong zilla longsid sk...  \n",
       "175  secret let cage t control stay away beast ugli...  "
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mellow_aggressive_sample = pd.concat([aggressive_songs, mellow_sample])\n",
    "mellow_aggressive_sample.sort_index(axis='index', inplace=True)\n",
    "mellow_aggressive_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aggressive    1683\n",
       "mellow        1683\n",
       "Name: moods, dtype: int64"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mellow_aggressive_sample['moods'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag Of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the bag of Words model to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 19678)\t1\n",
      "  (0, 9504)\t1\n",
      "  (0, 12305)\t3\n",
      "  (0, 10388)\t1\n",
      "  (0, 5128)\t1\n",
      "  (0, 2258)\t1\n",
      "  (0, 13568)\t1\n",
      "  (0, 2839)\t1\n",
      "  (0, 3696)\t1\n",
      "  (0, 10356)\t1\n",
      "  (0, 10839)\t2\n",
      "  (0, 17841)\t2\n",
      "  (0, 978)\t1\n",
      "  (0, 19813)\t1\n",
      "  (0, 15555)\t1\n",
      "  (0, 1319)\t1\n",
      "  (0, 18424)\t1\n",
      "  (0, 7392)\t1\n",
      "  (0, 18954)\t1\n",
      "  (0, 13973)\t9\n",
      "  (0, 17893)\t3\n",
      "  (0, 1550)\t3\n",
      "  (0, 8112)\t3\n",
      "  (0, 6393)\t3\n",
      "  (0, 6193)\t1\n",
      "  (0, 9719)\t2\n",
      "  (0, 760)\t4\n",
      "  (0, 10248)\t2\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = count_vect.fit_transform(mellow_aggressive_sample['clean_lyrics'])\n",
    "print(bag_of_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to look at several classifiers: Logistic Regression, SVC and Random Forests to try and get the best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow = bag_of_words\n",
    "y = mellow_aggressive_sample['moods']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7151335311572701\n"
     ]
    }
   ],
   "source": [
    "lr_bow = LogisticRegression(max_iter=5000)\n",
    "lr_bow.fit(X_train, y_train)\n",
    "lr_predictions = lr_bow.predict(X_test)\n",
    "print(accuracy_score(y_test, lr_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW LR - GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to optimize the parameters with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=5000, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'solver': ['newton-cg', 'sag', 'saga', 'lbfgs'], 'multi_class': ['ovr', 'multinomial']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'solver': ['newton-cg', 'sag', 'saga', 'lbfgs'], 'multi_class':['ovr', 'multinomial']}\n",
    "grid = GridSearchCV(lr_bow, param_grid, verbose=1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multi_class': 'ovr', 'solver': 'sag'}"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=5000, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7062314540059347\n"
     ]
    }
   ],
   "source": [
    "lr_predictions2 = grid.best_estimator_.predict(X_test)\n",
    "print(accuracy_score(y_test, lr_predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW LR - SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the number of features that can be used. Considering the high number of features, we will start with SelectFromModel as it can provide an optimized number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_bow3 = SelectFromModel(lr_bow2, prefit=True)\n",
    "X_train3 = lr_bow3.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (3029, 20445)\n",
      "X_train3 shape:  (3029, 5737)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_train3 shape: ', X_train3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try with 5,737 features instead of 20,445. This should help reduce the calculating time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337, 5737)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test3 = lr_bow3.transform(X_test)\n",
    "X_test3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.712166172106825\n"
     ]
    }
   ],
   "source": [
    "lr_bow4 = LogisticRegression(solver='sag', multi_class='ovr', max_iter=5000)\n",
    "lr_bow4.fit(X_train3, y_train)\n",
    "lr_predictions4 = lr_bow4.predict(X_test3)\n",
    "print(accuracy_score(y_test, lr_predictions4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is now similar to what we had originally with linear regression and BOW however the calculating time was reduced significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW LR - RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to optimize the features using RFE method instead of SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_bow5 = LogisticRegression(solver='sag', multi_class='ovr', max_iter=3000)\n",
    "#rfe_model = RFE(lr_bow5, 5736)\n",
    "#rfe_model = rfe_model.fit(X_train, y_train)\n",
    "#lr_predictions5 = rfe_model.predict(X_test)\n",
    "#print(accuracy_score(y_test, lr_predictions5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not done for now as the processing time is too long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now work with another classifier: Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44807121661721067\n"
     ]
    }
   ],
   "source": [
    "svc_bow = SVC(C=1, gamma=1)\n",
    "svc_bow.fit(X_train, y_train)\n",
    "svc_predictions = svc_bow.predict(X_test)\n",
    "print(accuracy_score(y_test, svc_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now notice quite a low score with our first trial of SVC, we need to adjust the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW SVC - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.1, 1, 10], 'gamma': [1, 0.1, 0.01, 0.001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'C': [0.1,1, 10], 'gamma': [1,0.1,0.01,0.001]}\n",
    "grid = GridSearchCV(svc_bow, param_grid, verbose=1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'gamma': 0.001}"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7151335311572701\n"
     ]
    }
   ],
   "source": [
    "svc_predictions2 = grid.best_estimator_.predict(X_test)\n",
    "print(accuracy_score(y_test, svc_predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the score improved quite significantly. Let's have a look at our features with SelectKBest. We are going to use the same number of features as in SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW SVC - SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3366, 5736)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = SelectKBest(k=5736)\n",
    "X_bow_new = selector.fit_transform(X_bow, y)\n",
    "X_bow_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_svc, X_test_svc, y_train_svc, y_test_svc = train_test_split(X_bow_new, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7418397626112759\n"
     ]
    }
   ],
   "source": [
    "svc_bow3 = SVC(C=1, gamma=0.001)\n",
    "svc_bow3.fit(X_train_svc, y_train_svc)\n",
    "svc_predictions3 = svc_bow3.predict(X_test_svc)\n",
    "print(accuracy_score(y_test_svc, svc_predictions3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is now even higher. This is for now the best result we got. Let's try to see if we can get a better model with RandomForests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW RandomForests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6201780415430267\n"
     ]
    }
   ],
   "source": [
    "rfc_bow = RandomForestClassifier(n_estimators=5, min_samples_split=2, max_features='log2')\n",
    "rfc_bow.fit(X_train, y_train)\n",
    "rfc_predictions = rfc_bow.predict(X_test)\n",
    "print(accuracy_score(y_test, rfc_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is quite low (but still a bit higher than for the SVC first trial). Let's work on parameters optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW RFC - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 45 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 135 out of 135 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='log2', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 100], 'min_samples_split': [2, 3, 4, 5, 10], 'max_features': ['sqrt', 'log2', 'auto']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'n_estimators': [5, 10, 100], 'min_samples_split': [2, 3, 4, 5, 10], 'max_features': ['sqrt', 'log2', 'auto']}\n",
    "grid = GridSearchCV(rfc_bow, param_grid, verbose=1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 100}"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=4,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7477744807121661\n"
     ]
    }
   ],
   "source": [
    "rfc_predictions2 = grid.best_estimator_.predict(X_test)\n",
    "print(accuracy_score(y_test, rfc_predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score increase significantly, let's check if something can be done by optimizing the number of features (with SelectFromModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW RFC - SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run SelectFromModel with threshold=mean (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (3029, 20445)\n",
      "X_train_rfc shape:  (3029, 2772)\n"
     ]
    }
   ],
   "source": [
    "rfc_bow3 = SelectFromModel(rfc_bow2, threshold='mean', prefit=True)\n",
    "X_train_rfc = rfc_bow3.transform(X_train)\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_train_rfc shape: ', X_train_rfc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337, 2772)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_rfc = rfc_bow3.transform(X_test)\n",
    "X_test_rfc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7329376854599406\n"
     ]
    }
   ],
   "source": [
    "rfc_bow4 = RandomForestClassifier(n_estimators=100, min_samples_split=5, max_features='sqrt')\n",
    "rfc_bow4.fit(X_train_rfc, y_train)\n",
    "rfc_predictions4 = rfc_bow4.predict(X_test_rfc)\n",
    "print(accuracy_score(y_test, rfc_predictions4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is very similar to the preivous step (can be slightly higher or slightly lower), however the number of features was divided by 7: which helps save calculation time. \n",
    "Let's quickly check what the results would be with the median as a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (3029, 20445)\n",
      "X_train_rfc shape:  (3029, 20445)\n"
     ]
    }
   ],
   "source": [
    "rfc_bow5 = SelectFromModel(rfc_bow2, threshold='median',prefit=True)\n",
    "X_train_rfc2 = rfc_bow5.transform(X_train)\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_train_rfc shape: ', X_train_rfc2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337, 20445)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_rfc2 = rfc_bow5.transform(X_test)\n",
    "X_test_rfc2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7655786350148368\n"
     ]
    }
   ],
   "source": [
    "rfc_bow6 = RandomForestClassifier(n_estimators=100, min_samples_split=5, max_features='sqrt')\n",
    "rfc_bow6.fit(X_train_rfc2, y_train)\n",
    "rfc_predictions6 = rfc_bow6.predict(X_test_rfc2)\n",
    "print(accuracy_score(y_test, rfc_predictions6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is also similar to the previous step however we can notice all the features were selected, meaning the threshold was not adapted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still notice that after all these steps the score is still not too high, this might be due to the initial choice of mood: aggressive vs. mellow.\n",
    "Let's now compare with a different technique TF-IDF and check whether we can get a higher score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the TF-IDF model to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idf_vect = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10248)\t0.04513957094995233\n",
      "  (0, 760)\t0.36761268311079304\n",
      "  (0, 9719)\t0.04085118350902226\n",
      "  (0, 6193)\t0.027647459075229572\n",
      "  (0, 6393)\t0.07667902713193415\n",
      "  (0, 8112)\t0.09098316220926433\n",
      "  (0, 1550)\t0.1252334343911038\n",
      "  (0, 17893)\t0.2757095123330948\n",
      "  (0, 13973)\t0.8271285369992843\n",
      "  (0, 18954)\t0.024608861033863328\n",
      "  (0, 7392)\t0.05771471402474821\n",
      "  (0, 18424)\t0.03260804010014442\n",
      "  (0, 1319)\t0.04289938627149122\n",
      "  (0, 15555)\t0.05039650326626615\n",
      "  (0, 19813)\t0.05870664208799416\n",
      "  (0, 978)\t0.07331512614662358\n",
      "  (0, 17841)\t0.05895434301780704\n",
      "  (0, 10839)\t0.15648686359079803\n",
      "  (0, 10356)\t0.03716026178393961\n",
      "  (0, 3696)\t0.05680554681381161\n",
      "  (0, 2839)\t0.028721337725565534\n",
      "  (0, 13568)\t0.07236638107029339\n",
      "  (0, 2258)\t0.05923863491043399\n",
      "  (0, 5128)\t0.021673086750183135\n",
      "  (0, 10388)\t0.024075784291750196\n",
      "  (0, 12305)\t0.08192076723413363\n",
      "  (0, 9504)\t0.02101512397139747\n",
      "  (0, 19678)\t0.04296047152898321\n"
     ]
    }
   ],
   "source": [
    "tf_idf = tf_idf_vect.fit_transform(mellow_aggressive_sample['clean_lyrics'])\n",
    "print(tf_idf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of going through all the classifiers, we will only do RandomForests and Support Vector Machine, as they were performing better. We will start with Random Forests to get the optimized number of features with SelectFromModel. We will later use this number of features with SelectKBest on SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF RandomForests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = tf_idf\n",
    "y = mellow_aggressive_sample['moods']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6706231454005934\n"
     ]
    }
   ],
   "source": [
    "rfc_tfidf = RandomForestClassifier(n_estimators=5, min_samples_split=2, max_features='log2')\n",
    "rfc_tfidf.fit(X_train, y_train)\n",
    "rfc_predictions = rfc_tfidf.predict(X_test)\n",
    "print(accuracy_score(y_test, rfc_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is low, parameters need to be adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF RF - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 45 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 135 out of 135 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='log2', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 100], 'min_samples_split': [2, 3, 4, 5, 10], 'max_features': ['sqrt', 'log2', 'auto']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'n_estimators': [5, 10, 100], 'min_samples_split': [2, 3, 4, 5, 10], 'max_features': ['sqrt', 'log2', 'auto']}\n",
    "grid = GridSearchCV(rfc_tfidf, param_grid, verbose=1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 'sqrt', 'min_samples_split': 3, 'n_estimators': 100}"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=3,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.744807121661721\n"
     ]
    }
   ],
   "source": [
    "rfc_predictions2 = grid.best_estimator_.predict(X_test)\n",
    "print(accuracy_score(y_test, rfc_predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score has increased. Let's now adjust the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF RFC - SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (3029, 20445)\n",
      "X_train_rfc shape:  (3029, 2498)\n"
     ]
    }
   ],
   "source": [
    "rfc_tfidf3 = SelectFromModel(rfc_tfidf2, threshold='mean', prefit=True)\n",
    "X_train_rfc = rfc_tfidf3.transform(X_train)\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_train_rfc shape: ', X_train_rfc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the number of features was reduced to 2466 in this case. We will use this number later on with SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337, 2498)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_rfc = rfc_tfidf3.transform(X_test)\n",
    "X_test_rfc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7388724035608308\n"
     ]
    }
   ],
   "source": [
    "rfc_tfidf4 = RandomForestClassifier(n_estimators=100, min_samples_split=10, max_features='sqrt')\n",
    "rfc_tfidf4.fit(X_train_rfc, y_train)\n",
    "rfc_predictions4 = rfc_tfidf4.predict(X_test_rfc)\n",
    "print(accuracy_score(y_test, rfc_predictions4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have significantly reduced the number of features however the score has decreased a little. This is a tradeoff that can be made to save calculation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look analyse the TF-IDF set with SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7685459940652819\n"
     ]
    }
   ],
   "source": [
    "svc_tfidf = SVC(C=1, gamma=1)\n",
    "svc_tfidf.fit(X_train, y_train)\n",
    "svc_predictions = svc_tfidf.predict(X_test)\n",
    "print(accuracy_score(y_test, svc_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice the initial score is already quite high (actually the highest one so far). Let's see if we can improve it even more by optimizing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF SVC - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.1, 1, 10], 'gamma': [1, 0.1, 0.01, 0.001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'C': [0.1,1, 10], 'gamma': [1,0.1,0.01,0.001]}\n",
    "grid = GridSearchCV(svc_tfidf, param_grid, verbose=1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'gamma': 1}"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7685459940652819\n"
     ]
    }
   ],
   "source": [
    "svc_predictions2 = grid.best_estimator_.predict(X_test)\n",
    "print(accuracy_score(y_test, svc_predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the parameters were already optimized, however let's see if we can get the same kind of score with a lower number of features, thus reducing the calculation time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF SVC - SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3366, 6000)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = SelectKBest(k=6000)\n",
    "X_tfidf_new = selector.fit_transform(X_tfidf, y)\n",
    "X_tfidf_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_svc, X_test_svc, y_train_svc, y_test_svc = train_test_split(X_tfidf_new, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8071216617210683\n"
     ]
    }
   ],
   "source": [
    "svc_tfidf3 = SVC(C=1, gamma=1)\n",
    "svc_tfidf3.fit(X_train_svc, y_train_svc)\n",
    "svc_predictions3 = svc_tfidf3.predict(X_test_svc)\n",
    "print(accuracy_score(y_test_svc, svc_predictions3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice I did not use the 2466 features vaue found with SelectFromModel. Instead I tried to modify manually the number of features and found an even better score with 6000 features: above 80%. This is the best score so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try one last thing: change the n-gram range on the best model: TF-IDF with SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF SVC with ngram range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 106202)\t0.03346675971056171\n",
      "  (0, 4631)\t0.2725503382800047\n",
      "  (0, 95445)\t0.030287322489270428\n",
      "  (0, 56145)\t0.020497998762641627\n",
      "  (0, 59678)\t0.05685030942605343\n",
      "  (0, 80391)\t0.06745548447371151\n",
      "  (0, 10922)\t0.09284885009523762\n",
      "  (0, 182483)\t0.2044127537100035\n",
      "  (0, 140355)\t0.6132382611300106\n",
      "  (0, 192769)\t0.0182451632046755\n",
      "  (0, 70182)\t0.04279004929337004\n",
      "  (0, 189370)\t0.024175804503632447\n",
      "  (0, 9349)\t0.031805872804382136\n",
      "  (0, 155994)\t0.037364282149129485\n",
      "  (0, 204622)\t0.0435254709521098\n",
      "  (0, 6170)\t0.054356292234566825\n",
      "  (0, 180829)\t0.043709117967880545\n",
      "  (0, 114331)\t0.11602033762038265\n",
      "  (0, 108829)\t0.02755085007971237\n",
      "  (0, 33392)\t0.042115987047211964\n",
      "  (0, 25061)\t0.021294179098270936\n",
      "  (0, 137299)\t0.05365288671191963\n",
      "  (0, 19726)\t0.043919893751920576\n",
      "  (0, 47113)\t0.016068561822590914\n",
      "  (0, 109392)\t0.01784993678005198\n",
      "  :\t:\n",
      "  (0, 192927)\t0.05512678580860656\n",
      "  (0, 70228)\t0.06813758457000117\n",
      "  (0, 189386)\t0.060730231836949554\n",
      "  (0, 9446)\t0.06813758457000117\n",
      "  (0, 156027)\t0.06813758457000117\n",
      "  (0, 140358)\t0.06813758457000117\n",
      "  (0, 204623)\t0.06813758457000117\n",
      "  (0, 6179)\t0.06813758457000117\n",
      "  (0, 181048)\t0.12971955753721123\n",
      "  (0, 114334)\t0.06813758457000117\n",
      "  (0, 108856)\t0.06813758457000117\n",
      "  (0, 33401)\t0.06813758457000117\n",
      "  (0, 25546)\t0.05512678580860656\n",
      "  (0, 114335)\t0.06813758457000117\n",
      "  (0, 137301)\t0.06813758457000117\n",
      "  (0, 19735)\t0.06813758457000117\n",
      "  (0, 47328)\t0.028276114282996766\n",
      "  (0, 95909)\t0.040642087950525034\n",
      "  (0, 109888)\t0.04740237032826081\n",
      "  (0, 127333)\t0.07090660037273648\n",
      "  (0, 127278)\t0.04912891698045279\n",
      "  (0, 4632)\t0.06813758457000117\n",
      "  (0, 4634)\t0.06813758457000117\n",
      "  (0, 92989)\t0.05512678580860656\n",
      "  (0, 201881)\t0.06485977876860562\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect_bigram = TfidfVectorizer(ngram_range=(1, 2))\n",
    "tf_idf_bigram = tf_idf_vect_bigram.fit_transform(mellow_aggressive_sample['clean_lyrics'])\n",
    "print(tf_idf_bigram[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF initial shape:  (3366, 20445)\n",
      "TF-IDF with ngram 2 shape:  (3366, 208924)\n"
     ]
    }
   ],
   "source": [
    "print('TF-IDF initial shape: ', tf_idf.shape)\n",
    "print('TF-IDF with ngram 2 shape: ', tf_idf_bigram.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose ngram_range = (1,2) meaning we will also be including bigrams. This is why we can notice the dimension significantly increased. Let's now have a look with SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_bigram = tf_idf_bigram\n",
    "y = mellow_aggressive_sample['moods']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_bigram, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7537091988130564\n"
     ]
    }
   ],
   "source": [
    "svc_tfidf_bigram = SVC(C=1, gamma=1)\n",
    "svc_tfidf_bigram.fit(X_train, y_train)\n",
    "svc_predictions_bigram = svc_tfidf_bigram.predict(X_test)\n",
    "print(accuracy_score(y_test, svc_predictions_bigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite a good initial score, let's try to get it even higher with parameters optimization (GridSearchCV). COnsidering the number of features this should be quite time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:  5.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.1, 1, 10], 'gamma': [1, 0.1, 0.01, 0.001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'C': [0.1,1, 10], 'gamma': [1,0.1,0.01,0.001]}\n",
    "grid = GridSearchCV(svc_tfidf_bigram, param_grid, verbose=1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'gamma': 1}"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predicted this step was quite time consuming (>5min) and we notice the parameters were already optimized. To reduce the calculation time, it might be interesting to reduce the number of features. Let's work on this with SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3366, 6000)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector_bigram = SelectKBest(k=6000)\n",
    "X_tfidf_bigram_new = selector_bigram.fit_transform(X_tfidf_bigram, y)\n",
    "X_tfidf_bigram_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bigram, X_test_bigram, y_train_bigram, y_test_bigram = train_test_split(X_tfidf_bigram_new, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8071216617210683\n"
     ]
    }
   ],
   "source": [
    "svc_tfidf_bigram2 = SVC(C=1, gamma=1)\n",
    "svc_tfidf_bigram2.fit(X_train_bigram, y_train_bigram)\n",
    "svc_predictions_bigram2 = svc_tfidf_bigram2.predict(X_test_bigram)\n",
    "print(accuracy_score(y_test_bigram, svc_predictions_bigram2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice there is the same score as previously (using only 1-grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Doc2VecHelperFunctions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_lyrics_to_d2v(lyrics_df['clean_lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('./song_lyrics.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our model looks like, what are the words related to 'good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bad', 0.5932339429855347),\n",
       " ('real', 0.5345109105110168),\n",
       " ('fine', 0.5313093066215515),\n",
       " ('right', 0.4795217216014862),\n",
       " ('better', 0.46048539876937866),\n",
       " ('damn', 0.45751577615737915),\n",
       " ('nice', 0.4475826621055603),\n",
       " ('pretti', 0.4366227388381958),\n",
       " ('wish', 0.43530163168907166),\n",
       " ('best', 0.43098145723342896)]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like our model was well trained as most of the words are linked to good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create our feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_features = []\n",
    "for index, row in mellow_aggressive_sample.iterrows():\n",
    "    song_index = 'SONG_NUMBER_' + str(index)\n",
    "    d2v_features.append(model[song_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3366, 100)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(d2v_features).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data use train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = d2v_features\n",
    "y = mellow_aggressive_sample['moods']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's analyze the data with RandomForests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D2V RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D2V RF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5934718100890207\n"
     ]
    }
   ],
   "source": [
    "rfc_d2v = RandomForestClassifier(n_estimators=5, min_samples_split=2, max_features='log2')\n",
    "rfc_d2v.fit(X_train, y_train)\n",
    "rfc_predictions = rfc_d2v.predict(X_test)\n",
    "print(accuracy_score(y_test, rfc_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is quite low, let's try to adjust the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D2V RF - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 45 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 135 out of 135 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='log2', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 100], 'min_samples_split': [2, 3, 4, 5, 10], 'max_features': ['sqrt', 'log2', 'auto']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'n_estimators': [5, 10, 100], 'min_samples_split': [2, 3, 4, 5, 10], 'max_features': ['sqrt', 'log2', 'auto']}\n",
    "grid = GridSearchCV(rfc_d2v, param_grid, verbose=1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 'sqrt', 'min_samples_split': 3, 'n_estimators': 100}"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=3,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7240356083086054\n"
     ]
    }
   ],
   "source": [
    "rfc_predictions2 = grid.best_estimator_.predict(X_test)\n",
    "print(accuracy_score(y_test, rfc_predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if changing the features make a difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D2V RF - SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (3029, 100)\n",
      "X_train_rfc shape:  (3029, 32)\n"
     ]
    }
   ],
   "source": [
    "rfc_d2v3 = SelectFromModel(rfc_d2v2, threshold='mean', prefit=True)\n",
    "X_train_d2v = rfc_d2v3.transform(X_train)\n",
    "print('X_train shape: ', np.asarray(X_train).shape)\n",
    "print('X_train_rfc shape: ', np.asarray(X_train_d2v).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337, 32)"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_d2v = rfc_d2v3.transform(X_test)\n",
    "np.asarray(X_test_d2v).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6735905044510386\n"
     ]
    }
   ],
   "source": [
    "rfc_d2v4 = RandomForestClassifier(n_estimators=100, min_samples_split=3, max_features='sqrt')\n",
    "rfc_d2v4.fit(X_train_d2v, y_train)\n",
    "rfc_predictions4 = rfc_d2v4.predict(X_test_d2v)\n",
    "print(accuracy_score(y_test, rfc_predictions4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is similar to the previous step. Let's look at other classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D2V LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7359050445103857\n"
     ]
    }
   ],
   "source": [
    "lr_d2v = LogisticRegression()\n",
    "lr_d2v.fit(X_train, y_train)\n",
    "lr_predictions = lr_d2v.predict(X_test)\n",
    "print(accuracy_score(y_test, lr_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is higher than with RFC, let's also adjust the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D2V LR - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'solver': ['newton-cg', 'sag', 'saga', 'lbfgs'], 'multi_class': ['ovr', 'multinomial']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'solver': ['newton-cg', 'sag', 'saga', 'lbfgs'], 'multi_class':['ovr', 'multinomial']}\n",
    "grid = GridSearchCV(lr_d2v, param_grid, verbose=1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multi_class': 'multinomial', 'solver': 'newton-cg'}"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7359050445103857\n"
     ]
    }
   ],
   "source": [
    "lr_predictions2 = grid.best_estimator_.predict(X_test)\n",
    "print(accuracy_score(y_test, lr_predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was no big change on the score after the parameter optimization. Let's try with Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D2V Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45103857566765576\n"
     ]
    }
   ],
   "source": [
    "svc_d2v = SVC(C=1, gamma=1)\n",
    "svc_d2v.fit(X_train, y_train)\n",
    "svc_predictions = svc_d2v.predict(X_test)\n",
    "print(accuracy_score(y_test, svc_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is very low with this classifier, we will now adjust the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D2V SVC - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:   45.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.1, 1, 10], 'gamma': [1, 0.1, 0.01, 0.001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'C': [0.1,1, 10], 'gamma': [1,0.1,0.01,0.001]}\n",
    "grid = GridSearchCV(svc_d2v, param_grid, verbose=1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'gamma': 0.01}"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7922848664688428\n"
     ]
    }
   ],
   "source": [
    "svc_predictions2 = grid.best_estimator_.predict(X_test)\n",
    "print(accuracy_score(y_test, svc_predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score increased, it's now higher than with the 2 other classifiers. We will now optimize the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D2V SVC - SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3366, 32)"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = SelectKBest(k=32)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_d2v, X_test_d2v, y_train_d2v, y_test_d2v = train_test_split(X_new, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7655786350148368\n"
     ]
    }
   ],
   "source": [
    "svc_d2v3 = SVC(C=1, gamma=0.01)\n",
    "svc_d2v3.fit(X_train_d2v, y_train_d2v)\n",
    "svc_predictions3 = svc_d2v3.predict(X_test_d2v)\n",
    "print(accuracy_score(y_test_d2v, svc_predictions3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score decreased slightly but with a smaller number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that brought in the best scores is TF-IDF with Support Vector Machine classifier. To increase the score we might want to try new classifiers or even with different moods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
